{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "df_aisles = pd.read_csv(path + 'aisles.csv')\n",
    "df_departments = pd.read_csv(path + 'departments.csv')\n",
    "df_data = pd.read_csv(path + 'order_products__prior.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'product_id': np.int32,\n",
    "    'add_to_cart_order': np.int16,\n",
    "    'reordered': np.int8,\n",
    "})\n",
    "df_train_target = pd.read_csv(path + 'order_products__train.csv')\n",
    "df_orders = pd.read_csv(path + 'orders.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'user_id': np.int32,\n",
    "    'order_number': np.int8,\n",
    "    'order_dow': np.int8,\n",
    "    'order_hour_of_day': np.int8,\n",
    "})\n",
    "df_products = pd.read_csv(path + 'products.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge arguments we will be using often\n",
    "merge_arguments = {\n",
    "    'left_index': True,\n",
    "    'right_index': True,\n",
    "    'how': 'outer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting index for easier mapping\n",
    "df_orders = df_orders.set_index('order_id')\n",
    "df_products = df_products.set_index('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# adding features to data to make it more complete\n",
    "# many of the values are repeated as a single user places many orders with many products\n",
    "df_data['user_id'] = df_data.order_id.map(df_orders.user_id)\n",
    "df_data['order_number'] = df_data.order_id.map(df_orders.order_number)\n",
    "df_data['order_dow'] = df_data.order_id.map(df_orders.order_dow)\n",
    "df_data['order_hour_of_day'] = df_data.order_id.map(df_orders.order_hour_of_day)\n",
    "df_data['days_since_prior_order'] = df_data.order_id.map(df_orders.days_since_prior_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of users in each group\n",
    "users_train = df_orders.loc[df_orders.eval_set == 'train', 'user_id']\n",
    "users_test = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "\n",
    "# create maps for convenience\n",
    "user_id_to_last_order_number = df_data.groupby('user_id').agg({'order_number': 'max'}).order_number\n",
    "user_id_to_last_order_id = df_orders.loc[~df_orders.user_id.duplicated(keep='last')].reset_index().set_index('user_id').order_id\n",
    "order_id_to_user_id = df_orders.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into two groups\n",
    "df_train = df_data.loc[df_data.user_id.isin(users_train)]\n",
    "df_test = df_data.loc[df_data.user_id.isin(users_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_target['user_id'] = df_train_target.order_id.map(order_id_to_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data: 32,434,489\n",
      "# of train data: 20,641,991\n",
      "# of test data: 11,792,498\n"
     ]
    }
   ],
   "source": [
    "# a quick look at the sizes\n",
    "print(\"# of data: {:,}\".format(df_data.shape[0]))\n",
    "print(\"# of train data: {:,}\".format(df_train.shape[0]))\n",
    "print(\"# of test data: {:,}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bestsellers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_sellers(df, quantile):\n",
    "    df_temp = df.groupby('product_id').agg({'order_id': 'count'}).rename(columns={'order_id': 'amount_sold'})\n",
    "    return df_temp.loc[df_temp.amount_sold >= df_temp.amount_sold.quantile(quantile)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we have seen in part 02,\n",
    "# top 20% of most sold products account for more than 90% of all items sold\n",
    "# there are approximately 50,000 different products sold\n",
    "# this should cut down the number of features down to about 10,000\n",
    "# as we shall be using one-hot encoded data for our model\n",
    "quantile = 0.8\n",
    "bestsellers = get_best_sellers(df_data, quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only bestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.product_id.isin(bestsellers)]\n",
    "df_test = df_test.loc[df_test.product_id.isin(bestsellers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in train data: 18,785,336\n",
      "# of bestsellers in test data: 10,723,251\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in train data: {:,}\".format(df_train.shape[0]))\n",
    "print(\"# of bestsellers in test data: {:,}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only last 3 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the minimum number of orders made by a user is 3\n",
    "# we cut off all orders prior to the last three\n",
    "# this is mainly done to have constant number of features across all users\n",
    "# after one-hot encoding all (products, order number) pairs\n",
    "# hopefully last 3 orders are relavent enough in predicting reorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not, we may do more feature engineering later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_history(df):\n",
    "    return df.loc[df.order_number <= df.user_id.map(user_id_to_last_order_number) - 3]\n",
    "\n",
    "def get_last_orders(df):\n",
    "    return df.loc[df.order_number > df.user_id.map(user_id_to_last_order_number) - 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might be used later at some point\n",
    "df_train_hist = get_history(df_train)\n",
    "df_test_hist = get_history(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = get_last_orders(df_train)\n",
    "df_test = get_last_orders(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers not in the last 3 train data: 15,162,662\n",
      "# of bestsellers not in the last 3 test data: 8,647,422\n",
      "\n",
      "# of bestsellers in the last 3 train data: 3,622,674\n",
      "# of bestsellers in the last 3 test data: 2,075,829\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers not in the last 3 train data: {:,}\".format(df_train_hist.shape[0]))\n",
    "print(\"# of bestsellers not in the last 3 test data: {:,}\".format(df_test_hist.shape[0]))\n",
    "print('')\n",
    "print(\"# of bestsellers in the last 3 train data: {:,}\".format(df_train.shape[0]))\n",
    "print(\"# of bestsellers in the last 3 test data: {:,}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here, we are throwing away about 80% of orders\n",
    "# perhaps this is too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_users(df, n):\n",
    "    if n > df.user_id.nunique():\n",
    "        return pd.Series(df.user_id.unique())\n",
    "    else:\n",
    "        return pd.Series(np.random.choice(df.user_id.unique(), n, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_users = get_sample_users(df_train, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_validation = df_train.loc[df_train.user_id.isin(validation_users)]\n",
    "df_validation_target = df_train_target.loc[df_train_target.user_id.isin(validation_users)]\n",
    "\n",
    "df_train = df_train.loc[~df_train.user_id.isin(validation_users)]\n",
    "df_train_target = df_train_target.loc[~df_train_target.user_id.isin(validation_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing functions and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function reduces order number down to 0, 1, or 2\n",
    "# these numbers are \"relative\" to the last order number made by a user\n",
    "# this is done so that we may one-hot encode the feature\n",
    "def standardize_order_number(df):\n",
    "    df.order_number = df.order_number - (df.user_id.map(user_id_to_last_order_number) - 2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_history_ohe_grouped_by_user(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded product feature and reordered feature\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id, prefix='prod'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id * df.reordered, prefix='re'), **merge_arguments)\n",
    "    \n",
    "    # group by order_number so that each row contains all the information\n",
    "    # on which products are ordered in that particular order\n",
    "    # we lose the information on when the products are added to the cart on that particular order\n",
    "    # but this information may not be relavent\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).sum()\n",
    "    \n",
    "    # unstack order_numbers so that each row now contains all the information\n",
    "    # on which products are ordered by a user in the last 3 orders\n",
    "    # fill_value is needed as there may be some users\n",
    "    # who did not order one of the bestsellers in a particular order among the last 3 orders\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded time related features\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_dow, prefix='dow'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_hour_of_day, prefix='hour'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.days_since_prior_order, prefix='days', dummy_na=True), **merge_arguments)\n",
    "        \n",
    "    # similar as above\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).max()\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function that returns column names in a predefined format\n",
    "# to make sure that we have a well defined format of a full list of features\n",
    "# otherwise, when using a partial set of users for our input,\n",
    "# we might be missing certain products\n",
    "def get_product_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        np.core.defchararray.add(str(i) + infix, bestsellers.astype(str))\n",
    "        for i in range(3) for infix in ['_prod_', '_re_']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        [str(i) + '_dow_' + str(dow) for dow in range(7) for i in range(3)],\n",
    "        [str(i) + '_hour_' + str(hour) for hour in range(24) for i in range(3)],\n",
    "        [str(i) + '_days_' + str(days) for days in np.sort(df_orders.days_since_prior_order.unique()) for i in range(3)],\n",
    "        ['last_' + str(last) for last in np.ceil(np.log(user_id_to_last_order_number.sort_values())).unique()]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_ohe_grouped_by_user(df):\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(np.ceil(np.log(df.user_id.map(user_id_to_last_order_number))), prefix='last'),\n",
    "                            **merge_arguments)\n",
    "    \n",
    "    df_temp = df_temp.groupby('user_id').max()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_columns():\n",
    "    return np.concatenate([get_time_ohe_columns(), get_product_ohe_columns()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple function to combine one-hot encoded product features and time related features\n",
    "def get_ohe_features(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=get_ohe_columns(), dtype=np.uint8)\n",
    "    \n",
    "    df_temp = get_product_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    df_temp = get_time_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    df_temp = get_last_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_target(df, users):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['user_id'] = df_temp.order_id.map(order_id_to_user_id)\n",
    "    \n",
    "    df_temp = df_temp.loc[df_temp.user_id.isin(users)]\n",
    "    \n",
    "    # take only those that are reorders of bestsellers\n",
    "    df_temp = df_temp.loc[df_temp.reordered == 1]\n",
    "    df_temp = df_temp.loc[df_temp.product_id.isin(bestsellers)]\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(df_temp.product_id), **merge_arguments)\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    df_temp = df_temp.drop(['order_id', 'product_id', 'add_to_cart_order', 'reordered'], axis=1)\n",
    "    \n",
    "    # group by users\n",
    "    df_temp = df_temp.groupby('user_id').sum()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_target(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=bestsellers, dtype=np.uint8)\n",
    "    df_temp = get_product_ohe_target(df, users)\n",
    "    \n",
    "    # add possible missing users back in to the target\n",
    "    missing_users = users.loc[~users.isin(df_temp.index)]\n",
    "    df_temp = df_temp.append([pd.DataFrame(data=0, index=missing_users, columns=df_temp.columns, dtype=np.uint8)])\n",
    "    \n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def set_predict(self, model):\n",
    "        sparse_predict_temp = []\n",
    "        for batch in self.user_batches:\n",
    "            predict_temp = pd.DataFrame(data=model.predict(self.sparse_features).astype(np.uint8).todense(),\n",
    "                                        index=batch, columns=bestsellers, dtype=np.uint8)\n",
    "            sparse_predict_temp.append(sp.sparse.csr_matrix(predict_temp))\n",
    "        self.sparse_predict = sp.sparse.vstack(sparse_predict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complete overhaul might be needed\n",
    "# somehow this code grown to accomodate\n",
    "class DF_ohe:\n",
    "    def __init__(self, df, users):\n",
    "        batch_size = 1024\n",
    "        \n",
    "        self.df = df.loc[df.user_id.isin(users)].copy()\n",
    "        self.df = standardize_order_number(self.df)\n",
    "        \n",
    "        # create small batches of users to save memory\n",
    "        self.users = users.sort_values()\n",
    "        self.users.index= range(self.users.size)\n",
    "        users_temp = self.users.copy()\n",
    "        self.user_batches = []\n",
    "        while users_temp.size > 0:\n",
    "            batch = users_temp[:batch_size]\n",
    "            users_temp = users_temp[batch_size:]\n",
    "            self.user_batches.append(batch)\n",
    "        \n",
    "        # possible missing users\n",
    "        # sample is taken after dataframe reduction\n",
    "        # we need to account for that\n",
    "        \n",
    "        # preprocess features in batches\n",
    "        # otherwise this will take up too much memory\n",
    "        sparse_features_temp = []\n",
    "        for batch in self.user_batches:\n",
    "            features_temp = get_ohe_features(self.df.loc[self.df.user_id.isin(batch)], batch)\n",
    "            sparse_features_temp.append(sp.sparse.csr_matrix(features_temp))\n",
    "        \n",
    "        self.sparse_features = sp.sparse.vstack(sparse_features_temp)\n",
    "        \n",
    "        # remove dense dataframe to free up memory\n",
    "        # del self.features\n",
    "        \n",
    "    def set_target(self, df):\n",
    "        # process transforming targets in batches\n",
    "        # otherwise this will take up too much memory\n",
    "        sparse_target_temp = []\n",
    "        for batch in self.user_batches:\n",
    "            target_temp = get_ohe_target(df.loc[df.user_id.isin(batch)], batch)\n",
    "            sparse_target_temp.append(sp.sparse.csr_matrix(target_temp))\n",
    "        self.sparse_target = sp.sparse.vstack(sparse_target_temp)\n",
    "    \n",
    "    def set_predict(self, model):\n",
    "        self.sparse_predict = model.predict(self.sparse_features)\n",
    "        \n",
    "    def set_submission(self):\n",
    "        dict_temp = dict()\n",
    "        \n",
    "        for i in range(self.sparse_predict.shape[0]):\n",
    "            reordered = bestsellers[self.sparse_predict[i].nonzero()[1]]\n",
    "            str_temp = ''\n",
    "            for j in range(len(reordered)):\n",
    "                if j == 0:\n",
    "                    str_temp += str(reordered[j])\n",
    "                else:\n",
    "                    str_temp += ' ' + str(reordered[j])\n",
    "            if str_temp == '':\n",
    "                str_temp = 'None'\n",
    "            dict_temp[str(self.users[i])] = str_temp\n",
    "        \n",
    "        self.submission = pd.DataFrame.from_dict(dict_temp, orient='index')\n",
    "        self.submission.index = self.submission.index.astype(np.uint32)\n",
    "        self.submission['order_id'] = user_id_to_last_order_id\n",
    "        self.submission.columns = ['products', 'order_id']\n",
    "        self.submission = self.submission.loc[:, ['order_id', 'products']]\n",
    "    \n",
    "    def print_results(self, verbose=False):\n",
    "        # number of predicted reorders\n",
    "        PT = self.sparse_predict.sum()\n",
    "        # number of relevant reorders\n",
    "        RT = self.sparse_target.sum()\n",
    "        # true positive of reorders\n",
    "        TP = self.sparse_predict.multiply(self.sparse_target).sum()\n",
    "        # number of false negative among bestsellers\n",
    "        FN1 = RT - TP\n",
    "        # estimation of false negative among non-bestsellers\n",
    "        FN2 = 0.1 * RT\n",
    "        # false positive of reorders\n",
    "        FP = PT - TP\n",
    "        \n",
    "        # print detailed result of verbose is true\n",
    "        if verbose:\n",
    "            print('predicted true: {}'.format(PT))\n",
    "            print('relevant true: {}'.format(RT))\n",
    "            print('true positive: {}'.format(TP))\n",
    "            print('false negative 1: {}'.format(FN1))\n",
    "            print('false negative 2: {}'.format(int(FN2)))\n",
    "            print('false positive: {}'.format(FP))\n",
    "            print('')\n",
    "        \n",
    "        print('precision score: {:.3f}'.format(TP / (TP + FP)))\n",
    "        print('pseudo f1 score: {:.3f}'.format((2 * TP) / ((2 * TP) + FN1 + FN2 + FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "sample_size = 1024\n",
    "sample_users = get_sample_users(df_train, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(DecisionTreeClassifier(criterion='entropy'), n_jobs=4)\n",
    "model_name = 'TS{:05d}_OVRDT'.format(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "df_ohe_sample.set_target(df_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "model.fit(df_ohe_sample.sparse_features, df_ohe_sample.sparse_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit/predict precision/score #1 #2 #3\n",
    "\n",
    "# for a sample size of 256\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   13s/0.9s 0.191/0.118 0.183/0.105 0.198/0.112\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   13s/0.9s 0.381/0.052 0.460/0.067 0.477/0.059\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   14s/0.9s 0.162/0.100 0.168/0.095 0.149/0.089\n",
    "# OvR + SGDClassifier:\n",
    "#   7s/0.8s 0.119/0.052 0.127/0.048 0.170/0.065\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   97s/3s  0.110/0.046 0.177/0.081 0.195/0.080\n",
    "\n",
    "# for a sample size of 512\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   44s/2s 0.167/0.117 0.177/0.126 0.167/0.118\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   45s/2s 0.407/0.109 0.443/0.113 0.381/0.099\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   46s/2s 0.171/0.126 0.175/0.126 0.171/0.126\n",
    "# OvR + SGDClassifier:\n",
    "#   9s/1s 0.225/0.076 0.184/0.070 0.185/0.058\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   464s/10s 0.196/0.110 0.194/0.108 0.227/0.127\n",
    "\n",
    "# for a sample size of 1024\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   74s/4s 0.190/0.140 0.179/0.133 0.190/0.139\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   74s/4s 0.412/0.140 0.399/0.126 0.437/0.138\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   82s/4s 0.194/0.152 0.185/0.144 0.185/0.145\n",
    "# OvR + SGDClassifier:\n",
    "#   14s/3s 0.256/0.093 0.257/0.091 0.294/0.101\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   1159s/31s 0.162/0.070 0.205/0.086 0.175/0.076\n",
    "\n",
    "# comment: note that although \"OvR + DecisionTreeClassifier with min leaf 5\" has\n",
    "# comparable score to other models, it is distinguished by having high precision rate\n",
    "# this means that the model is more cautious in marking a product as reordered\n",
    "\n",
    "# comment: there doesn't seem to be any difference in regards to calculation time\n",
    "# between different parameters\n",
    "\n",
    "# OvR + DecisionTreeClassifier with max depth n: might need further investigation\n",
    "# OvR + DecisionTreeClassifier with max features n: very low score\n",
    "# OvR + SGDClassifier: fast but less accurate than decision trees\n",
    "# OvR + AdaBoostClassifier: quite slow compared to decision trees\n",
    "# OvR + Perceptron: fast but less accurate than decision trees\n",
    "#   at sample size of 1024, there is a huge dip in its score for some reason\n",
    "\n",
    "# OvR + SGDClassifier: the model itself takes up too much memory\n",
    "# OvR + SVC: takes too long\n",
    "# OvR + LinearSVC: does not predict\n",
    "# OvR + XGBClassifier: takes too long\n",
    "# OvR + KNeighborsClassifier: does not predict\n",
    "# OvR + RandomForestClassifier: does not predict\n",
    "# OvR + MLPClassifier: takes too long\n",
    "# OvR + BernoulliNB: does not predict\n",
    "# OvR + GradientBoostingClassifier: does not work with sparse form\n",
    "# OvR + ExtraTreesClassifier: low score\n",
    "# KNeighborsClassifier: does not work in sparse form\n",
    "# DecisionTreeClassifier: low score, does not work with sparse form\n",
    "# RandomForestClassifier: does not work with sparse form\n",
    "# MLPClassifier: does not predict\n",
    "# MO + anything: does not work with sparse form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 5719\n",
      "relevant true: 5719\n",
      "true positive: 5719\n",
      "false negative 1: 0\n",
      "false negative 2: 571\n",
      "false positive: 0\n",
      "\n",
      "precision score: 1.000\n",
      "pseudo f1 score: 0.952\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample.set_predict(model)\n",
    "df_ohe_sample.print_results(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set validation data\n",
    "df_ohe_validation = DF_ohe(df_validation, validation_users)\n",
    "df_ohe_validation.set_target(df_validation_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 3036\n",
      "relevant true: 12359\n",
      "true positive: 277\n",
      "false negative 1: 12082\n",
      "false negative 2: 1235\n",
      "false positive: 2759\n",
      "\n",
      "precision score: 0.091\n",
      "pseudo f1 score: 0.033\n",
      "Wall time: 467 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_validation.set_predict(model)\n",
    "df_ohe_validation.print_results(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('TS32768_OVRDTSLSSMD5.pkl')\n",
    "model_name = 'TS32768_OVRDTSLSSMD5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting ready\n",
    "test_users = pd.Series(df_test.user_id.unique())\n",
    "all_test_users = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "missing_test_users = all_test_users.loc[~all_test_users.isin(test_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create user batches\n",
    "batch_size = 1024\n",
    "user_batches = []\n",
    "\n",
    "while test_users.size > 0:\n",
    "    batch = test_users[:batch_size]\n",
    "    test_users = test_users[batch_size:]\n",
    "    user_batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there may be some missing users who did not purchase\n",
    "# any product from the bestsellers list\n",
    "# we need to keep track of them to add them back in\n",
    "# in the final result at the end\n",
    "missing_test_users_submission = pd.DataFrame(index=missing_test_users)\n",
    "missing_test_users_submission['order_id'] = user_id_to_last_order_id\n",
    "missing_test_users_submission['products'] = 'None'\n",
    "\n",
    "# matching index data type to DF_ohe.submission.index\n",
    "missing_test_users_submission.index = missing_test_users_submission.index.astype(np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 \n",
      "Wall time: 26min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "submission = pd.DataFrame(columns=['order_id', 'products'])\n",
    "submission.index = submission.index.astype(np.uint64)\n",
    "\n",
    "for batch in user_batches:\n",
    "    print(str(i), end=' ')\n",
    "    i += 1\n",
    "    df_ohe_test = DF_ohe(df_test, batch)\n",
    "    df_ohe_test.set_predict(model)\n",
    "    df_ohe_test.set_submission()\n",
    "    submission = submission.append(df_ohe_test.submission)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding in missing users and sort the result by order_id\n",
    "submission = submission.append(missing_test_users_submission)\n",
    "submission = submission.sort_values(by='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export to a file\n",
    "submission.to_csv(model_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for multiple trials, if we wanted to be more rigorous,\n",
    "# on each trial, we should be taking new samples for fitting\n",
    "# however, we do not implement this as this would take much more time to process\n",
    "def model_test(model, model_name, sample_size=128, verbose=False, pickle=False):\n",
    "    print('================================')\n",
    "    print('TS{:05d}_'.format(sample_size) + model_name)\n",
    "    print('================================')\n",
    "    \n",
    "    # get a sample and a fit\n",
    "    sample_users = get_sample_users(df_train, sample_size)\n",
    "    df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "    df_ohe_sample.set_target(df_train_target)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Fitting')\n",
    "    %time model.fit(df_ohe_sample.sparse_features, df_ohe_sample.sparse_target)\n",
    "    if verbose:\n",
    "        print('--------------------------------')\n",
    "    \n",
    "    # pickle the model for later use if pickle is true\n",
    "    if pickle:\n",
    "        joblib.dump(model, 'TS{:05d}_'.format(sample_size) + model_name + '.pkl')\n",
    "    \n",
    "    # sample_validation_users = get_sample_users(df_train, sample_size)\n",
    "    if verbose:\n",
    "        print('Validating')\n",
    "    %time df_ohe_validation.set_predict(model)\n",
    "    if verbose:\n",
    "        print('--------------------------------')\n",
    "\n",
    "    # print results\n",
    "    df_ohe_validation.print_results(verbose)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "TS00032_OVRDT\n",
      "================================\n",
      "Fitting\n",
      "Wall time: 7.12 s\n",
      "--------------------------------\n",
      "Validating\n",
      "Wall time: 536 ms\n",
      "--------------------------------\n",
      "predicted true: 3636\n",
      "relevant true: 12359\n",
      "true positive: 331\n",
      "false negative 1: 12028\n",
      "false negative 2: 1235\n",
      "false positive: 3305\n",
      "\n",
      "precision score: 0.091\n",
      "pseudo f1 score: 0.038\n",
      "\n",
      "================================\n",
      "TS00032_OVRDT1_8\n",
      "================================\n",
      "Fitting\n",
      "Wall time: 7.23 s\n",
      "--------------------------------\n",
      "Validating\n",
      "Wall time: 513 ms\n",
      "--------------------------------\n",
      "predicted true: 5491\n",
      "relevant true: 12359\n",
      "true positive: 423\n",
      "false negative 1: 11936\n",
      "false negative 2: 1235\n",
      "false positive: 5068\n",
      "\n",
      "precision score: 0.077\n",
      "pseudo f1 score: 0.044\n",
      "\n",
      "================================\n",
      "TS00032_OVRDT1_16\n",
      "================================\n",
      "Fitting\n",
      "Wall time: 5.67 s\n",
      "--------------------------------\n",
      "Validating\n",
      "Wall time: 458 ms\n",
      "--------------------------------\n",
      "predicted true: 3156\n",
      "relevant true: 12359\n",
      "true positive: 342\n",
      "false negative 1: 12017\n",
      "false negative 2: 1235\n",
      "false positive: 2814\n",
      "\n",
      "precision score: 0.108\n",
      "pseudo f1 score: 0.041\n",
      "\n",
      "================================\n",
      "TS00032_OVRDT1_24\n",
      "================================\n",
      "Fitting\n",
      "Wall time: 6.45 s\n",
      "--------------------------------\n",
      "Validating\n",
      "Wall time: 491 ms\n",
      "--------------------------------\n",
      "predicted true: 2935\n",
      "relevant true: 12359\n",
      "true positive: 210\n",
      "false negative 1: 12149\n",
      "false negative 2: 1235\n",
      "false positive: 2725\n",
      "\n",
      "precision score: 0.072\n",
      "pseudo f1 score: 0.025\n",
      "\n",
      "================================\n",
      "TS00032_OVRDT1_32\n",
      "================================\n",
      "Fitting\n",
      "Wall time: 6.48 s\n",
      "--------------------------------\n",
      "Validating\n",
      "Wall time: 477 ms\n",
      "--------------------------------\n",
      "predicted true: 2601\n",
      "relevant true: 12359\n",
      "true positive: 372\n",
      "false negative 1: 11987\n",
      "false negative 2: 1235\n",
      "false positive: 2229\n",
      "\n",
      "precision score: 0.143\n",
      "pseudo f1 score: 0.046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# setting test arguments\n",
    "verbose = True\n",
    "pickle = False\n",
    "sample_sizes = [32]\n",
    "models = [\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier()), 'OVR_DT'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(min_samples_leaf=5)), 'OVR_DT_SL5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(min_samples_split=5)), 'OVR_DT_SS5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(max_depth=5)), 'OVR_DT_MD5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(min_samples_leaf=5, min_samples_split=5, max_depth=5)), 'OVR_DT_SLSSMD5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(criterion='entropy', class_weight={0:1, 1:3})), 'OVRDT'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(criterion='entropy', class_weight={0:1, 1:3}, min_samples_leaf=5)), 'OVRDTSL5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(criterion='entropy', class_weight={0:1, 1:3}, min_samples_split=5)), 'OVRDTSS5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(criterion='entropy', class_weight={0:1, 1:3}, max_depth=5)), 'OVRDTMD5'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier()), 'OVRDT'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:2})), 'OVRDT1_2'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:3})), 'OVRDT1_3'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:4})), 'OVRDT1_4'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:5})), 'OVRDT1_5'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:6})), 'OVRDT1_6'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:7})), 'OVRDT1_7'),\n",
    "#    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:8})), 'OVRDT1_8'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:8})), 'OVRDT1_8'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:16})), 'OVRDT1_16'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:24})), 'OVRDT1_24'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(class_weight={0:1, 1:32})), 'OVRDT1_32'),\n",
    "]\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    for model, model_name in models:\n",
    "        model_test(model, model_name, sample_size, verbose, pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "saved_model_files = glob.glob('*.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do\n",
    "----\n",
    "* add more features (order history)\n",
    "* optimize parameters\n",
    "* append missing users\n",
    "* try with more products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Incremental Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encode the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
