{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aisles = pd.read_csv(path + 'aisles.csv')\n",
    "df_departments = pd.read_csv(path + 'departments.csv')\n",
    "df_data = pd.read_csv(path + 'order_products__prior.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'product_id': np.int32,\n",
    "    'add_to_cart_order': np.int16,\n",
    "    'reordered': np.int8,\n",
    "})\n",
    "df_train_target = pd.read_csv(path + 'order_products__train.csv')\n",
    "df_orders = pd.read_csv(path + 'orders.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'user_id': np.int32,\n",
    "    'order_number': np.int8,\n",
    "    'order_dow': np.int8,\n",
    "    'order_hour_of_day': np.int8,\n",
    "})\n",
    "df_products = pd.read_csv(path + 'products.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge arguments we will be using often\n",
    "merge_arguments = {\n",
    "    'left_index': True,\n",
    "    'right_index': True,\n",
    "    'how': 'outer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting index for easier mapping\n",
    "df_orders = df_orders.set_index('order_id')\n",
    "df_products = df_products.set_index('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# adding features to data to make it more complete\n",
    "# many of the values are repeated as a single user places many orders with many products\n",
    "df_data['user_id'] = df_data.order_id.map(df_orders.user_id)\n",
    "df_data['order_number'] = df_data.order_id.map(df_orders.order_number)\n",
    "df_data['order_dow'] = df_data.order_id.map(df_orders.order_dow)\n",
    "df_data['order_hour_of_day'] = df_data.order_id.map(df_orders.order_hour_of_day)\n",
    "df_data['days_since_prior_order'] = df_data.order_id.map(df_orders.days_since_prior_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of users in each group\n",
    "users_train = df_orders.loc[df_orders.eval_set == 'train', 'user_id']\n",
    "users_test = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "\n",
    "# create maps for convenience\n",
    "user_to_last_order = df_data.groupby('user_id').agg({'order_number': 'max'}).order_number\n",
    "user_to_order = df_orders.loc[~df_orders.user_id.duplicated(keep='last')].reset_index().set_index('user_id').order_id\n",
    "order_to_user = df_orders.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into two groups\n",
    "df_train = df_data.loc[df_data.user_id.isin(users_train)]\n",
    "df_test = df_data.loc[df_data.user_id.isin(users_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data: (32434489, 9)\n",
      "# of train data: (20641991, 9)\n",
      "# of test data: (11792498, 9)\n"
     ]
    }
   ],
   "source": [
    "# a quick look at the sizes\n",
    "print(\"# of data: {}\".format(df_data.shape))\n",
    "print(\"# of train data: {}\".format(df_train.shape))\n",
    "print(\"# of test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bestsellers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_sellers(df, quantile):\n",
    "    df_temp = df.groupby('product_id').agg({'order_id': 'count'}).rename(columns={'order_id': 'amount_sold'})\n",
    "    return df_temp.loc[df_temp.amount_sold >= df_temp.amount_sold.quantile(quantile)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we have seen in part 02,\n",
    "# top 20% of most sold products account for more than 90% of all items sold\n",
    "# there are approximately 50,000 different products sold\n",
    "# this should cut down the number of features down to about 10,000\n",
    "# as we shall be using one-hot encoded data for our model\n",
    "quantile = 0.8\n",
    "bestsellers = get_best_sellers(df_data, quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only bestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.product_id.isin(bestsellers)]\n",
    "df_test = df_test.loc[df_test.product_id.isin(bestsellers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in train data: (18785336, 9)\n",
      "# of bestsellers in test data: (10723251, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in train data: {}\".format(df_train.shape))\n",
    "print(\"# of bestsellers in test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only last 3 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the minimum number of orders made by a user is 3\n",
    "# we cut off all orders prior to the last three\n",
    "# this is mainly done to have constant number of features across all users\n",
    "# after one-hot encoding all (products, order number) pairs\n",
    "# hopefully last 3 orders are relavent enough in predicting reorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not, we may do more feature engineering later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_orders(df):\n",
    "    return df.loc[df.user_id.map(user_to_last_order) - df.order_number < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = get_last_orders(df_train)\n",
    "df_test = get_last_orders(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in the last 3 train data: (3622674, 9)\n",
      "# of bestsellers in the last 3 test data: (2075829, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in the last 3 train data: {}\".format(df_train.shape))\n",
    "print(\"# of bestsellers in the last 3 test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing functions and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function reduces order number down to 0, 1, or 2\n",
    "# these numbers are \"relative\" to the last order number made by a user\n",
    "# this is done so that we may one-hot encode the feature\n",
    "def standardize_order_number(df):\n",
    "    df.order_number = df.order_number - (df.user_id.map(user_to_last_order) - 2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_history_ohe_grouped_by_user(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded product feature and reordered feature\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id, prefix='prod'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id * df.reordered, prefix='re'), **merge_arguments)\n",
    "    \n",
    "    # group by order_number so that each row contains all the information\n",
    "    # on which products are ordered in that particular order\n",
    "    # we lose the information on when the products are added to the cart on that particular order\n",
    "    # but this information may not be relavent\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).sum()\n",
    "    \n",
    "    # unstack order_numbers so that each row now contains all the information\n",
    "    # on which products are ordered by a user in the last 3 orders\n",
    "    # fill_value is needed as there may be some users\n",
    "    # who did not order one of the bestsellers in a particular order among the last 3 orders\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded time related features\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_dow, prefix='dow'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_hour_of_day, prefix='hour'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.days_since_prior_order, prefix='days', dummy_na=True), **merge_arguments)\n",
    "        \n",
    "    # similar as above\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).max()\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function that returns column names in a predefined format\n",
    "# to make sure that we have a well defined format of a full list of features\n",
    "# otherwise, when using a partial set of users for our input,\n",
    "# we might be missing certain products\n",
    "def get_product_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        np.core.defchararray.add(str(i) + infix, bestsellers.astype(str))\n",
    "        for i in range(3) for infix in ['_prod_', '_re_']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        [str(i) + '_dow_' + str(dow) for dow in range(7) for i in range(3)],\n",
    "        [str(i) + '_hour_' + str(hour) for hour in range(24) for i in range(3)],\n",
    "        [str(i) + '_days_' + str(days) for days in np.sort(df_orders.days_since_prior_order.unique()) for i in range(3)],\n",
    "        ['last_' + str(last) for last in np.ceil(np.log(user_to_last_order.sort_values())).unique()]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_ohe_grouped_by_user(df):\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(np.ceil(np.log(df.user_id.map(user_to_last_order))), prefix='last'),\n",
    "                            **merge_arguments)\n",
    "    \n",
    "    df_temp = df_temp.groupby('user_id').max()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_columns():\n",
    "    return np.concatenate([get_time_ohe_columns(), get_product_ohe_columns()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple function to combine one-hot encoded product features and time related features\n",
    "def get_ohe_features(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=get_ohe_columns(), dtype=np.uint8)\n",
    "    \n",
    "    df_temp = get_product_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    df_temp = get_time_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    df_temp = get_last_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_target(df, users):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['user_id'] = df_temp.order_id.map(order_to_user)\n",
    "    \n",
    "    df_temp = df_temp.loc[df_temp.user_id.isin(users)]\n",
    "    \n",
    "    # take only those that are reorders of bestsellers\n",
    "    df_temp = df_temp.loc[df_temp.reordered == 1]\n",
    "    df_temp = df_temp.loc[df_temp.product_id.isin(bestsellers)]\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(df_temp.product_id), **merge_arguments)\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    df_temp = df_temp.drop(['order_id', 'product_id', 'add_to_cart_order', 'reordered'], axis=1)\n",
    "    \n",
    "    # group by users\n",
    "    df_temp = df_temp.groupby('user_id').sum()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_target(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=bestsellers, dtype=np.uint8)\n",
    "    df_temp = get_product_ohe_target(df, users)\n",
    "    \n",
    "    # add possible missing users back in to the target\n",
    "    missing_users = users.loc[~users.isin(df_temp.index)]\n",
    "    df_temp = df_temp.append([pd.DataFrame(data=0, index=missing_users, columns=df_temp.columns, dtype=np.uint8)])\n",
    "    \n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DF_ohe:\n",
    "    def __init__(self, df, users):\n",
    "        self.df = df.loc[df.user_id.isin(users)].copy()\n",
    "        self.df = standardize_order_number(self.df)\n",
    "        \n",
    "        # possible missing users\n",
    "        # sample is take after dataframe reduction\n",
    "        # we need to account for that\n",
    "        self.features = get_ohe_features(self.df, users)\n",
    "        self.sparse_features = sp.sparse.csc_matrix(self.features)\n",
    "        \n",
    "    def set_target(self, df, users):\n",
    "        self.target = get_ohe_target(df, users)\n",
    "        self.sparse_target = sp.sparse.csr_matrix(self.target)\n",
    "    \n",
    "    def set_predict(self, model):\n",
    "        self.predict = pd.DataFrame(data=model.predict(self.sparse_features).astype(np.uint8).todense(),\n",
    "                                    index=self.features.index, columns=bestsellers, dtype=np.uint8)\n",
    "    \n",
    "    def set_submission(self):\n",
    "        dict_temp = dict()\n",
    "        \n",
    "        for index, row in self.predict.iterrows():\n",
    "            reordered = bestsellers[row.nonzero()]\n",
    "            str_temp = ''\n",
    "            for j in range(len(reordered)):\n",
    "                if j == 0:\n",
    "                    str_temp += str(reordered[j])\n",
    "                else:\n",
    "                    str_temp += ' ' + str(reordered[j])\n",
    "            if str_temp == '':\n",
    "                str_temp = 'None'\n",
    "            dict_temp[str(index)] = str_temp\n",
    "        \n",
    "        self.submission = pd.DataFrame.from_dict(dict_temp, orient='index')\n",
    "        self.submission.index = self.submission.index.astype(np.uint32)\n",
    "        self.submission['order_id'] = user_to_order\n",
    "        self.submission.columns = ['products', 'order_id']\n",
    "        self.submission = self.submission.loc[:, ['order_id', 'products']]\n",
    "    \n",
    "    def print_results(self):\n",
    "        # number of predicted reorders\n",
    "        PT = self.predict.sum().sum()\n",
    "        # number of relevant reorders\n",
    "        RT = self.target.sum().sum()\n",
    "        # true positive of reorders\n",
    "        TP = self.predict.multiply(self.target).sum().sum()\n",
    "        # number of false negative among bestsellers\n",
    "        FN1 = RT - TP\n",
    "        # estimation of false negative among non-bestsellers\n",
    "        FN2 = 0.1 * RT\n",
    "        # false positive of reorders\n",
    "        FP = PT - TP\n",
    "        \n",
    "        print('predicted true: {}'.format(PT))\n",
    "        print('relevant true: {}'.format(RT))\n",
    "        print('true positive: {}'.format(TP))\n",
    "        print('false negative 1: {}'.format(FN1))\n",
    "        print('false negative 2: {}'.format(int(FN2)))\n",
    "        print('false positive: {}'.format(FP))\n",
    "        print('')\n",
    "        print('precision score: {:.3f}'.format(TP / (TP + FP)))\n",
    "        print('pseudo f1 score: {:.3f}'.format((2 * TP) / ((2 * TP) + FN1 + FN2 + FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a sample of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_users(df, n):\n",
    "    return pd.Series(np.random.choice(df.user_id.unique(), n, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "sample_size = 256\n",
    "sample_users = get_sample_users(df_train, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "df_ohe_sample.set_target(df_train_target, sample_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,320,832\n"
     ]
    }
   ],
   "source": [
    "print('{:,}'.format(df_ohe_sample.features.memory_usage().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if time and memory used are in linear relation to ths number of users,\n",
    "# as there are about 130,000 users in the training data,\n",
    "# this should take little more than an hour\n",
    "# and should take up about 60GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparsifying the dataframes takes more than 2 minutes\n",
    "# for a sample of 100 users\n",
    "# although this reduces the memory usage down to 1/50 of\n",
    "# its dense form, this would take more than 2 days to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# besides, due to pandas not being able to merge sparse data properly\n",
    "# (i.e., even with sparse dataframe as its input, its output is always a dense dataframe)\n",
    "# memory usage is still an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append missing users\n",
    "--------\n",
    "There may be missing users after the whole process. Namely, those who did not order any products from bestsellers list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "model.fit(df_ohe_sample.sparse_features, df_ohe_sample.sparse_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit/predict precision/score #1 #2 #3\n",
    "\n",
    "# for a sample size of 256\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   13s/24s 0.166/0.106 0.144/0.083 0.169/0.107\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   11s/22s 0.433/0.075 0.548/0.078 0.436/0.066\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   14s/25s 0.119/0.075 0.144/0.084 0.169/0.107\n",
    "\n",
    "# for a sample size of 512\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   27s/81s 0.156/0.121 0.191/0.137 0.168/0.122\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   27s/82s 0.440/0.111 0.378/0.103 0.426/0.119\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   27s/81s 0.187/0.132 0.155/0.113 0.172/0.124\n",
    "\n",
    "# for a sample size of 1024\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   76s/405s 0.178/0.142 0.168/0.133 0.168/0.135\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   68s/391s 0.382/0.131 0.437/0.150 0.440/0.150\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   72s/394s 0.187/0.149 0.181/0.143 0.182/0.140\n",
    "\n",
    "# comment: note that although \"OvR + DecisionTreeClassifier with min leaf 5\" has\n",
    "# comparable score to other models, it is distinguished by having high precision rate\n",
    "# this means that the model is more cautious in marking a product as reordered\n",
    "\n",
    "# comment: there doesn't seem to be any difference in regards to calculation time\n",
    "# between different parameters\n",
    "\n",
    "# OvR + SGDClassifier: ???\n",
    "# OvR + AdaBoostClassifier: quite slow compared to decision trees\n",
    "\n",
    "# OvR + SVC: takes too long\n",
    "# OvR + LinearSVC: does not predict\n",
    "# OvR + XGBClassifier: takes too long\n",
    "# OvR + KNeighborsClassifier: does not predict\n",
    "# OvR + RandomForestClassifier: does not predict\n",
    "# OvR + MLPClassifier: takes too long\n",
    "# OvR + BernoulliNB: does not predict\n",
    "# KNeighborsClassifier: does not work in sparse form\n",
    "# DecisionTreeClassifier: low score, does not work with sparse form\n",
    "# RandomForestClassifier: does not work with sparse form\n",
    "# MLPClassifier: does not predict\n",
    "# MO + anything: does not work with sparse form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a sample size of 100\n",
    "# OvR + SGDClassifier:\n",
    "#   fit: 64s predict: 33s score: 0.072 0.063 0.050\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   fit: 126s predict: 38s score: 0.070, 0.112, 0.066\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   fit: 41s predict: 7s precision/score: 0.149/0.074 0.215/0.091 0.144/0.061\n",
    "# DecisionTreeClassifier:\n",
    "#   fit: 30s predict: 0s score: 0.055 0.036 0.080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 1505\n",
      "relevant true: 1505\n",
      "true positive: 1505\n",
      "false negative 1: 0\n",
      "false negative 2: 150\n",
      "false positive: 0\n",
      "\n",
      "precision score: 1.000\n",
      "pseudo f1 score: 0.952\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample.set_predict(model)\n",
    "df_ohe_sample.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joblib.dump(model, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate against another sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "validation_size = 256\n",
    "sample_validation_users = get_sample_users(df_train, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 758\n",
      "relevant true: 1456\n",
      "true positive: 119\n",
      "false negative 1: 1337\n",
      "false negative 2: 145\n",
      "false positive: 639\n",
      "\n",
      "precision score: 0.157\n",
      "pseudo f1 score: 0.101\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_validation = DF_ohe(df_train, sample_validation_users)\n",
    "df_ohe_validation.set_target(df_train_target, sample_validation_users)\n",
    "df_ohe_validation.set_predict(model)\n",
    "df_ohe_validation.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ohe_validation.set_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test(model, model_name, sample_size=100, trials=3):\n",
    "    print('================================================================')\n",
    "    print(model_name + ' with sample size {}'.format(sample_size))\n",
    "    print('================================================================')\n",
    "    # get a sample and fit\n",
    "    sample_users = get_sample_users(df_train, sample_size)\n",
    "    df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "    df_ohe_sample.set_target(df_train_target, sample_users)\n",
    "    %time model.fit(df_ohe_sample.sparse_features, df_ohe_sample.sparse_target)\n",
    "    \n",
    "    for i in range(trials):\n",
    "        print('Trial #{}'.format(i))\n",
    "        print('--------------------------------')\n",
    "        # validate model on a new sample\n",
    "        sample_validation_users = get_sample_users(df_train, sample_size)\n",
    "        df_ohe_validation = DF_ohe(df_train, sample_validation_users)\n",
    "        df_ohe_validation.set_target(df_train_target, sample_validation_users)\n",
    "        %time df_ohe_validation.set_predict(model)\n",
    "\n",
    "        # print results\n",
    "        df_ohe_validation.print_results()\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "OvR + DTC with sample size 32\n",
      "================================================================\n",
      "Wall time: 5.88 s\n",
      "Trial #0\n",
      "--------------------------------\n",
      "Wall time: 1.24 s\n",
      "predicted true: 24\n",
      "relevant true: 148\n",
      "true positive: 3\n",
      "false negative 1: 145\n",
      "false negative 2: 14\n",
      "false positive: 21\n",
      "\n",
      "precision score: 0.125\n",
      "pseudo f1 score: 0.032\n",
      "\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 1.17 s\n",
      "predicted true: 34\n",
      "relevant true: 159\n",
      "true positive: 2\n",
      "false negative 1: 157\n",
      "false negative 2: 15\n",
      "false positive: 32\n",
      "\n",
      "precision score: 0.059\n",
      "pseudo f1 score: 0.019\n",
      "\n",
      "Trial #2\n",
      "--------------------------------\n",
      "Wall time: 1.16 s\n",
      "predicted true: 52\n",
      "relevant true: 218\n",
      "true positive: 5\n",
      "false negative 1: 213\n",
      "false negative 2: 21\n",
      "false positive: 47\n",
      "\n",
      "precision score: 0.096\n",
      "pseudo f1 score: 0.034\n",
      "\n",
      "================================================================\n",
      "OvR + DTC.SL(5) with sample size 32\n",
      "================================================================\n",
      "Wall time: 6.94 s\n",
      "Trial #0\n",
      "--------------------------------\n",
      "Wall time: 1.11 s\n",
      "predicted true: 15\n",
      "relevant true: 160\n",
      "true positive: 8\n",
      "false negative 1: 152\n",
      "false negative 2: 16\n",
      "false positive: 7\n",
      "\n",
      "precision score: 0.533\n",
      "pseudo f1 score: 0.084\n",
      "\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 1.11 s\n",
      "predicted true: 15\n",
      "relevant true: 187\n",
      "true positive: 7\n",
      "false negative 1: 180\n",
      "false negative 2: 18\n",
      "false positive: 8\n",
      "\n",
      "precision score: 0.467\n",
      "pseudo f1 score: 0.063\n",
      "\n",
      "Trial #2\n",
      "--------------------------------\n",
      "Wall time: 1.11 s\n",
      "predicted true: 13\n",
      "relevant true: 156\n",
      "true positive: 4\n",
      "false negative 1: 152\n",
      "false negative 2: 15\n",
      "false positive: 9\n",
      "\n",
      "precision score: 0.308\n",
      "pseudo f1 score: 0.043\n",
      "\n",
      "================================================================\n",
      "OvR + DTC.SS(5) with sample size 32\n",
      "================================================================\n",
      "Wall time: 5.45 s\n",
      "Trial #0\n",
      "--------------------------------\n",
      "Wall time: 852 ms\n",
      "predicted true: 37\n",
      "relevant true: 181\n",
      "true positive: 4\n",
      "false negative 1: 177\n",
      "false negative 2: 18\n",
      "false positive: 33\n",
      "\n",
      "precision score: 0.108\n",
      "pseudo f1 score: 0.034\n",
      "\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 833 ms\n",
      "predicted true: 42\n",
      "relevant true: 186\n",
      "true positive: 7\n",
      "false negative 1: 179\n",
      "false negative 2: 18\n",
      "false positive: 35\n",
      "\n",
      "precision score: 0.167\n",
      "pseudo f1 score: 0.057\n",
      "\n",
      "Trial #2\n",
      "--------------------------------\n",
      "Wall time: 833 ms\n",
      "predicted true: 43\n",
      "relevant true: 195\n",
      "true positive: 5\n",
      "false negative 1: 190\n",
      "false negative 2: 19\n",
      "false positive: 38\n",
      "\n",
      "precision score: 0.116\n",
      "pseudo f1 score: 0.039\n",
      "\n",
      "================================================================\n",
      "OvR + DTC with sample size 64\n",
      "================================================================\n",
      "Wall time: 9.74 s\n",
      "Trial #0\n",
      "--------------------------------\n",
      "Wall time: 3.44 s\n",
      "predicted true: 109\n",
      "relevant true: 430\n",
      "true positive: 22\n",
      "false negative 1: 408\n",
      "false negative 2: 43\n",
      "false positive: 87\n",
      "\n",
      "precision score: 0.202\n",
      "pseudo f1 score: 0.076\n",
      "\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 3.44 s\n",
      "predicted true: 100\n",
      "relevant true: 366\n",
      "true positive: 22\n",
      "false negative 1: 344\n",
      "false negative 2: 36\n",
      "false positive: 78\n",
      "\n",
      "precision score: 0.220\n",
      "pseudo f1 score: 0.088\n",
      "\n",
      "Trial #2\n",
      "--------------------------------\n",
      "Wall time: 3.6 s\n",
      "predicted true: 95\n",
      "relevant true: 331\n",
      "true positive: 15\n",
      "false negative 1: 316\n",
      "false negative 2: 33\n",
      "false positive: 80\n",
      "\n",
      "precision score: 0.158\n",
      "pseudo f1 score: 0.065\n",
      "\n",
      "================================================================\n",
      "OvR + DTC.SL(5) with sample size 64\n",
      "================================================================\n",
      "Wall time: 10.1 s\n",
      "Trial #0\n",
      "--------------------------------\n",
      "Wall time: 3.54 s\n",
      "predicted true: 29\n",
      "relevant true: 431\n",
      "true positive: 15\n",
      "false negative 1: 416\n",
      "false negative 2: 43\n",
      "false positive: 14\n",
      "\n",
      "precision score: 0.517\n",
      "pseudo f1 score: 0.060\n",
      "\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 3.59 s\n",
      "predicted true: 17\n",
      "relevant true: 357\n",
      "true positive: 6\n",
      "false negative 1: 351\n",
      "false negative 2: 35\n",
      "false positive: 11\n",
      "\n",
      "precision score: 0.353\n",
      "pseudo f1 score: 0.029\n",
      "\n",
      "Trial #2\n",
      "--------------------------------\n",
      "Wall time: 3.78 s\n",
      "predicted true: 25\n",
      "relevant true: 351\n",
      "true positive: 9\n",
      "false negative 1: 342\n",
      "false negative 2: 35\n",
      "false positive: 16\n",
      "\n",
      "precision score: 0.360\n",
      "pseudo f1 score: 0.044\n",
      "\n",
      "================================================================\n",
      "OvR + DTC.SS(5) with sample size 64\n",
      "================================================================\n",
      "Wall time: 11 s\n",
      "Trial #0\n",
      "--------------------------------\n",
      "Wall time: 4.17 s\n",
      "predicted true: 148\n",
      "relevant true: 389\n",
      "true positive: 21\n",
      "false negative 1: 368\n",
      "false negative 2: 38\n",
      "false positive: 127\n",
      "\n",
      "precision score: 0.142\n",
      "pseudo f1 score: 0.073\n",
      "\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 4.08 s\n",
      "predicted true: 117\n",
      "relevant true: 334\n",
      "true positive: 5\n",
      "false negative 1: 329\n",
      "false negative 2: 33\n",
      "false positive: 112\n",
      "\n",
      "precision score: 0.043\n",
      "pseudo f1 score: 0.021\n",
      "\n",
      "Trial #2\n",
      "--------------------------------\n",
      "Wall time: 4.18 s\n",
      "predicted true: 178\n",
      "relevant true: 400\n",
      "true positive: 18\n",
      "false negative 1: 382\n",
      "false negative 2: 40\n",
      "false positive: 160\n",
      "\n",
      "precision score: 0.101\n",
      "pseudo f1 score: 0.058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "sample_size = 32\n",
    "model_test(OneVsRestClassifier(DecisionTreeClassifier()),\n",
    "           'OvR + DTC', sample_size, trials=3)\n",
    "model_test(OneVsRestClassifier(DecisionTreeClassifier(min_samples_leaf=5)),\n",
    "           'OvR + DTC.SL(5)', sample_size, trials=3)\n",
    "model_test(OneVsRestClassifier(DecisionTreeClassifier(min_samples_split=5)),\n",
    "           'OvR + DTC.SS(5)', sample_size, trials=3)\n",
    "\n",
    "sample_size = 64\n",
    "model_test(OneVsRestClassifier(DecisionTreeClassifier()),\n",
    "           'OvR + DTC', sample_size, trials=3)\n",
    "model_test(OneVsRestClassifier(DecisionTreeClassifier(min_samples_leaf=5)),\n",
    "           'OvR + DTC.SL(5)', sample_size, trials=3)\n",
    "model_test(OneVsRestClassifier(DecisionTreeClassifier(min_samples_split=5)),\n",
    "           'OvR + DTC.SS(5)', sample_size, trials=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the test data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_users = df_test.user_id.unique()\n",
    "all_test_users = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "missing_test_users = all_test_users.loc[~all_test_users.isin(test_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_batches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while test_users.size > 0:\n",
    "    batch = test_users[:100]\n",
    "    test_users = test_users[100:]\n",
    "    user_batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_test_users_submission = pd.DataFrame(index=missing_test_users)\n",
    "missing_test_users_submission['order_id'] = user_to_order\n",
    "missing_test_users_submission['products'] = 'None'\n",
    "\n",
    "# matching index data type to DF_ohe.submission.index\n",
    "missing_test_users_submission.index = missing_test_users_submission.index.astype(np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submission = pd.DataFrame(columns=['order_id', 'products'])\n",
    "submission.index = submission.index.astype(np.uint64)\n",
    "for batch in user_batches[:10]:\n",
    "    df_ohe_test = DF_ohe(df_test, batch)\n",
    "    df_ohe_test.set_predict(model)\n",
    "    df_ohe_test.set_submission()\n",
    "    submission = submission.append(df_ohe_test.submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = submission.append(missing_test_users_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = submission.sort_values(by='order_id')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do\n",
    "----\n",
    "* sparsify data\n",
    "* add more features (order history)\n",
    "* optimize parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Incremental Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encode the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
