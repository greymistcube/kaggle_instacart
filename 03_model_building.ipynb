{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aisles = pd.read_csv(path + 'aisles.csv')\n",
    "df_departments = pd.read_csv(path + 'departments.csv')\n",
    "df_data = pd.read_csv(path + 'order_products__prior.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'product_id': np.int32,\n",
    "    'add_to_cart_order': np.int16,\n",
    "    'reordered': np.int8,\n",
    "})\n",
    "df_train_target = pd.read_csv(path + 'order_products__train.csv')\n",
    "df_orders = pd.read_csv(path + 'orders.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'user_id': np.int32,\n",
    "    'order_number': np.int8,\n",
    "    'order_dow': np.int8,\n",
    "    'order_hour_of_day': np.int8,\n",
    "})\n",
    "df_products = pd.read_csv(path + 'products.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge arguments we will be using often\n",
    "merge_arguments = {\n",
    "    'left_index': True,\n",
    "    'right_index': True,\n",
    "    'how': 'outer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting index for easier mapping\n",
    "df_orders = df_orders.set_index('order_id')\n",
    "df_products = df_products.set_index('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# adding features to data to make it more complete\n",
    "# many of the values are repeated as a single user places many orders with many products\n",
    "df_data['user_id'] = df_data.order_id.map(df_orders.user_id)\n",
    "df_data['order_number'] = df_data.order_id.map(df_orders.order_number)\n",
    "df_data['order_dow'] = df_data.order_id.map(df_orders.order_dow)\n",
    "df_data['order_hour_of_day'] = df_data.order_id.map(df_orders.order_hour_of_day)\n",
    "df_data['days_since_prior_order'] = df_data.order_id.map(df_orders.days_since_prior_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of users in each group\n",
    "users_train = df_orders.loc[df_orders.eval_set == 'train', 'user_id']\n",
    "users_test = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "\n",
    "# create maps for convenience\n",
    "user_to_last_order = df_data.groupby('user_id').agg({'order_number': 'max'}).order_number\n",
    "user_to_order = df_orders.loc[~df_orders.user_id.duplicated(keep='last')].reset_index().set_index('user_id').order_id\n",
    "order_to_user = df_orders.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_target['user_id'] = df_train_target.order_id.map(order_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into two groups\n",
    "df_train = df_data.loc[df_data.user_id.isin(users_train)]\n",
    "df_test = df_data.loc[df_data.user_id.isin(users_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data: (32434489, 9)\n",
      "# of train data: (20641991, 9)\n",
      "# of test data: (11792498, 9)\n"
     ]
    }
   ],
   "source": [
    "# a quick look at the sizes\n",
    "print(\"# of data: {}\".format(df_data.shape))\n",
    "print(\"# of train data: {}\".format(df_train.shape))\n",
    "print(\"# of test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bestsellers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_sellers(df, quantile):\n",
    "    df_temp = df.groupby('product_id').agg({'order_id': 'count'}).rename(columns={'order_id': 'amount_sold'})\n",
    "    return df_temp.loc[df_temp.amount_sold >= df_temp.amount_sold.quantile(quantile)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we have seen in part 02,\n",
    "# top 20% of most sold products account for more than 90% of all items sold\n",
    "# there are approximately 50,000 different products sold\n",
    "# this should cut down the number of features down to about 10,000\n",
    "# as we shall be using one-hot encoded data for our model\n",
    "quantile = 0.8\n",
    "bestsellers = get_best_sellers(df_data, quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only bestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.product_id.isin(bestsellers)]\n",
    "df_test = df_test.loc[df_test.product_id.isin(bestsellers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in train data: (18785336, 9)\n",
      "# of bestsellers in test data: (10723251, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in train data: {}\".format(df_train.shape))\n",
    "print(\"# of bestsellers in test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only last 3 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the minimum number of orders made by a user is 3\n",
    "# we cut off all orders prior to the last three\n",
    "# this is mainly done to have constant number of features across all users\n",
    "# after one-hot encoding all (products, order number) pairs\n",
    "# hopefully last 3 orders are relavent enough in predicting reorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not, we may do more feature engineering later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_orders(df):\n",
    "    return df.loc[df.user_id.map(user_to_last_order) - df.order_number < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = get_last_orders(df_train)\n",
    "df_test = get_last_orders(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in the last 3 train data: (3622674, 9)\n",
      "# of bestsellers in the last 3 test data: (2075829, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in the last 3 train data: {}\".format(df_train.shape))\n",
    "print(\"# of bestsellers in the last 3 test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing functions and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function reduces order number down to 0, 1, or 2\n",
    "# these numbers are \"relative\" to the last order number made by a user\n",
    "# this is done so that we may one-hot encode the feature\n",
    "def standardize_order_number(df):\n",
    "    df.order_number = df.order_number - (df.user_id.map(user_to_last_order) - 2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_history_ohe_grouped_by_user(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded product feature and reordered feature\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id, prefix='prod'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id * df.reordered, prefix='re'), **merge_arguments)\n",
    "    \n",
    "    # group by order_number so that each row contains all the information\n",
    "    # on which products are ordered in that particular order\n",
    "    # we lose the information on when the products are added to the cart on that particular order\n",
    "    # but this information may not be relavent\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).sum()\n",
    "    \n",
    "    # unstack order_numbers so that each row now contains all the information\n",
    "    # on which products are ordered by a user in the last 3 orders\n",
    "    # fill_value is needed as there may be some users\n",
    "    # who did not order one of the bestsellers in a particular order among the last 3 orders\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded time related features\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_dow, prefix='dow'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_hour_of_day, prefix='hour'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.days_since_prior_order, prefix='days', dummy_na=True), **merge_arguments)\n",
    "        \n",
    "    # similar as above\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).max()\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function that returns column names in a predefined format\n",
    "# to make sure that we have a well defined format of a full list of features\n",
    "# otherwise, when using a partial set of users for our input,\n",
    "# we might be missing certain products\n",
    "def get_product_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        np.core.defchararray.add(str(i) + infix, bestsellers.astype(str))\n",
    "        for i in range(3) for infix in ['_prod_', '_re_']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        [str(i) + '_dow_' + str(dow) for dow in range(7) for i in range(3)],\n",
    "        [str(i) + '_hour_' + str(hour) for hour in range(24) for i in range(3)],\n",
    "        [str(i) + '_days_' + str(days) for days in np.sort(df_orders.days_since_prior_order.unique()) for i in range(3)],\n",
    "        ['last_' + str(last) for last in np.ceil(np.log(user_to_last_order.sort_values())).unique()]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_ohe_grouped_by_user(df):\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(np.ceil(np.log(df.user_id.map(user_to_last_order))), prefix='last'),\n",
    "                            **merge_arguments)\n",
    "    \n",
    "    df_temp = df_temp.groupby('user_id').max()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_columns():\n",
    "    return np.concatenate([get_time_ohe_columns(), get_product_ohe_columns()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple function to combine one-hot encoded product features and time related features\n",
    "def get_ohe_features(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=get_ohe_columns(), dtype=np.uint8)\n",
    "    \n",
    "    df_temp = get_product_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    df_temp = get_time_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    df_temp = get_last_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_target(df, users):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['user_id'] = df_temp.order_id.map(order_to_user)\n",
    "    \n",
    "    df_temp = df_temp.loc[df_temp.user_id.isin(users)]\n",
    "    \n",
    "    # take only those that are reorders of bestsellers\n",
    "    df_temp = df_temp.loc[df_temp.reordered == 1]\n",
    "    df_temp = df_temp.loc[df_temp.product_id.isin(bestsellers)]\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(df_temp.product_id), **merge_arguments)\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    df_temp = df_temp.drop(['order_id', 'product_id', 'add_to_cart_order', 'reordered'], axis=1)\n",
    "    \n",
    "    # group by users\n",
    "    df_temp = df_temp.groupby('user_id').sum()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_target(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=bestsellers, dtype=np.uint8)\n",
    "    df_temp = get_product_ohe_target(df, users)\n",
    "    \n",
    "    # add possible missing users back in to the target\n",
    "    missing_users = users.loc[~users.isin(df_temp.index)]\n",
    "    df_temp = df_temp.append([pd.DataFrame(data=0, index=missing_users, columns=df_temp.columns, dtype=np.uint8)])\n",
    "    \n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def set_predict(self, model):\n",
    "        sparse_predict_temp = []\n",
    "        for batch in self.user_batches:\n",
    "            predict_temp = pd.DataFrame(data=model.predict(self.sparse_features).astype(np.uint8).todense(),\n",
    "                                        index=batch, columns=bestsellers, dtype=np.uint8)\n",
    "            sparse_predict_temp.append(sp.sparse.csr_matrix(predict_temp))\n",
    "        self.sparse_predict = sp.sparse.vstack(sparse_predict_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete overhaul might be needed\n",
    "# somehow this code grown to accomodate\n",
    "class DF_ohe:\n",
    "    def __init__(self, df, users):\n",
    "        batch_size = 1024\n",
    "        \n",
    "        self.df = df.loc[df.user_id.isin(users)].copy()\n",
    "        self.df = standardize_order_number(self.df)\n",
    "        \n",
    "        # create small batches of users to save memory\n",
    "        self.users = users.sort_values()\n",
    "        self.users.index= range(self.users.size)\n",
    "        users_temp = self.users.copy()\n",
    "        self.user_batches = []\n",
    "        while users_temp.size > 0:\n",
    "            batch = users_temp[:batch_size]\n",
    "            users_temp = users_temp[batch_size:]\n",
    "            self.user_batches.append(batch)\n",
    "        \n",
    "        # possible missing users\n",
    "        # sample is taken after dataframe reduction\n",
    "        # we need to account for that\n",
    "        \n",
    "        # preprocess features in batches\n",
    "        # otherwise this will take up too much memory\n",
    "        sparse_features_temp = []\n",
    "        for batch in self.user_batches:\n",
    "            features_temp = get_ohe_features(self.df.loc[self.df.user_id.isin(batch)], batch)\n",
    "            sparse_features_temp.append(sp.sparse.csr_matrix(features_temp))\n",
    "        \n",
    "        self.sparse_features = sp.sparse.vstack(sparse_features_temp)\n",
    "        \n",
    "        # remove dense dataframe to free up memory\n",
    "        # del self.features\n",
    "        \n",
    "    def set_target(self, df):\n",
    "        # process transforming targets in batches\n",
    "        # otherwise this will take up too much memory\n",
    "        sparse_target_temp = []\n",
    "        for batch in self.user_batches:\n",
    "            target_temp = get_ohe_target(df.loc[df.user_id.isin(batch)], batch)\n",
    "            sparse_target_temp.append(sp.sparse.csr_matrix(target_temp))\n",
    "        self.sparse_target = sp.sparse.vstack(sparse_target_temp)\n",
    "    \n",
    "    def set_predict(self, model):\n",
    "        self.sparse_predict = model.predict(self.sparse_features)\n",
    "        \n",
    "    def set_submission(self):\n",
    "        dict_temp = dict()\n",
    "        \n",
    "        for i in range(self.sparse_predict.shape[0]):\n",
    "            reordered = bestsellers[self.sparse_predict[i].nonzero()[1]]\n",
    "            str_temp = ''\n",
    "            for j in range(len(reordered)):\n",
    "                if j == 0:\n",
    "                    str_temp += str(reordered[j])\n",
    "                else:\n",
    "                    str_temp += ' ' + str(reordered[j])\n",
    "            if str_temp == '':\n",
    "                str_temp = 'None'\n",
    "            dict_temp[str(self.users[i])] = str_temp\n",
    "        \n",
    "        self.submission = pd.DataFrame.from_dict(dict_temp, orient='index')\n",
    "        self.submission.index = self.submission.index.astype(np.uint32)\n",
    "        self.submission['order_id'] = user_to_order\n",
    "        self.submission.columns = ['products', 'order_id']\n",
    "        self.submission = self.submission.loc[:, ['order_id', 'products']]\n",
    "    \n",
    "    def print_results(self, verbose=False):\n",
    "        # number of predicted reorders\n",
    "        PT = self.sparse_predict.sum()\n",
    "        # number of relevant reorders\n",
    "        RT = self.sparse_target.sum()\n",
    "        # true positive of reorders\n",
    "        TP = self.sparse_predict.multiply(self.sparse_target).sum()\n",
    "        # number of false negative among bestsellers\n",
    "        FN1 = RT - TP\n",
    "        # estimation of false negative among non-bestsellers\n",
    "        FN2 = 0.1 * RT\n",
    "        # false positive of reorders\n",
    "        FP = PT - TP\n",
    "        \n",
    "        # print detailed result of verbose is true\n",
    "        if verbose:\n",
    "            print('predicted true: {}'.format(PT))\n",
    "            print('relevant true: {}'.format(RT))\n",
    "            print('true positive: {}'.format(TP))\n",
    "            print('false negative 1: {}'.format(FN1))\n",
    "            print('false negative 2: {}'.format(int(FN2)))\n",
    "            print('false positive: {}'.format(FP))\n",
    "            print('')\n",
    "        \n",
    "        print('precision score: {:.3f}'.format(TP / (TP + FP)))\n",
    "        print('pseudo f1 score: {:.3f}'.format((2 * TP) / ((2 * TP) + FN1 + FN2 + FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.externals import joblib\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_users(df, n):\n",
    "    return pd.Series(np.random.choice(df.user_id.unique(), n, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "sample_size = 2048\n",
    "sample_users = get_sample_users(df_train, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(DecisionTreeClassifier(max_features=5), n_jobs=4)\n",
    "model_name = 'TS{:05d}_OVRDTMF5'.format(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "df_ohe_sample.set_target(df_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "model.fit(df_ohe_sample.sparse_features, df_ohe_sample.sparse_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit/predict precision/score #1 #2 #3\n",
    "\n",
    "# for a sample size of 256\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   13s/0.9s 0.191/0.118 0.183/0.105 0.198/0.112\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   13s/0.9s 0.381/0.052 0.460/0.067 0.477/0.059\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   14s/0.9s 0.162/0.100 0.168/0.095 0.149/0.089\n",
    "# OvR + SGDClassifier:\n",
    "#   7s/0.8s 0.119/0.052 0.127/0.048 0.170/0.065\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   97s/3s  0.110/0.046 0.177/0.081 0.195/0.080\n",
    "\n",
    "# for a sample size of 512\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   44s/2s 0.167/0.117 0.177/0.126 0.167/0.118\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   45s/2s 0.407/0.109 0.443/0.113 0.381/0.099\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   46s/2s 0.171/0.126 0.175/0.126 0.171/0.126\n",
    "# OvR + SGDClassifier:\n",
    "#   9s/1s 0.225/0.076 0.184/0.070 0.185/0.058\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   464s/10s 0.196/0.110 0.194/0.108 0.227/0.127\n",
    "\n",
    "# for a sample size of 1024\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   74s/4s 0.190/0.140 0.179/0.133 0.190/0.139\n",
    "# OvR + DecisionTreeClassifier with min leaf 5:\n",
    "#   74s/4s 0.412/0.140 0.399/0.126 0.437/0.138\n",
    "# OvR + DecisionTreeClassifier with min split 5:\n",
    "#   82s/4s 0.194/0.152 0.185/0.144 0.185/0.145\n",
    "# OvR + SGDClassifier:\n",
    "#   14s/3s 0.256/0.093 0.257/0.091 0.294/0.101\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   1159s/31s 0.162/0.070 0.205/0.086 0.175/0.076\n",
    "\n",
    "# comment: note that although \"OvR + DecisionTreeClassifier with min leaf 5\" has\n",
    "# comparable score to other models, it is distinguished by having high precision rate\n",
    "# this means that the model is more cautious in marking a product as reordered\n",
    "\n",
    "# comment: there doesn't seem to be any difference in regards to calculation time\n",
    "# between different parameters\n",
    "\n",
    "# OvR + DecisionTreeClassifier with max depth n: might need further investigation\n",
    "# OvR + DecisionTreeClassifier with max features n: very low score\n",
    "# OvR + SGDClassifier: fast but less accurate than decision trees\n",
    "# OvR + AdaBoostClassifier: quite slow compared to decision trees\n",
    "#   at sample size of 1024, there is a huge dip in its score for some reason\n",
    "\n",
    "# OvR + SGDClassifier: the model itself takes up too much memory\n",
    "# OvR + SVC: takes too long\n",
    "# OvR + LinearSVC: does not predict\n",
    "# OvR + XGBClassifier: takes too long\n",
    "# OvR + KNeighborsClassifier: does not predict\n",
    "# OvR + RandomForestClassifier: does not predict\n",
    "# OvR + MLPClassifier: takes too long\n",
    "# OvR + BernoulliNB: does not predict\n",
    "# OvR + GradientBoostingClassifier: does not work with sparse form\n",
    "# OvR + ExtraTreesClassifier: low score\n",
    "# KNeighborsClassifier: does not work in sparse form\n",
    "# DecisionTreeClassifier: low score, does not work with sparse form\n",
    "# RandomForestClassifier: does not work with sparse form\n",
    "# MLPClassifier: does not predict\n",
    "# MO + anything: does not work with sparse form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 11786\n",
      "relevant true: 11786\n",
      "true positive: 11786\n",
      "false negative 1: 0\n",
      "false negative 2: 1178\n",
      "false positive: 0\n",
      "\n",
      "precision score: 1.000\n",
      "pseudo f1 score: 0.952\n",
      "Wall time: 9.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample.set_predict(model)\n",
    "df_ohe_sample.print_results(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate against another sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "validation_size = 2048\n",
    "sample_validation_users = get_sample_users(df_train, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 8648\n",
      "relevant true: 12045\n",
      "true positive: 568\n",
      "false negative 1: 11477\n",
      "false negative 2: 1204\n",
      "false positive: 8080\n",
      "\n",
      "precision score: 0.066\n",
      "pseudo f1 score: 0.052\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_validation = DF_ohe(df_train, sample_validation_users)\n",
    "df_ohe_validation.set_target(df_train_target)\n",
    "df_ohe_validation.set_predict(model)\n",
    "df_ohe_validation.print_results(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1864787</td>\n",
       "      <td>5876 36316 41757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>878518</td>\n",
       "      <td>18127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1131395</td>\n",
       "      <td>13997 19057 27104 45446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>985905</td>\n",
       "      <td>18610 30588 34423 42447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>2956359</td>\n",
       "      <td>17896 18027 20383 21137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1652713</td>\n",
       "      <td>23452 27790 39277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>3081810</td>\n",
       "      <td>5449 6184 8518 13176 19634 24852 39928 48442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2000464</td>\n",
       "      <td>5750 8087 12606 13237 24654 31231 47626 48299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>417305</td>\n",
       "      <td>8048 21903 47226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1614375</td>\n",
       "      <td>2846 2966 23734 25513 47766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     order_id                                       products\n",
       "43    1864787                               5876 36316 41757\n",
       "116    878518                                          18127\n",
       "217   1131395                        13997 19057 27104 45446\n",
       "271    985905                        18610 30588 34423 42447\n",
       "313   2956359                        17896 18027 20383 21137\n",
       "345   1652713                              23452 27790 39277\n",
       "355   3081810   5449 6184 8518 13176 19634 24852 39928 48442\n",
       "470   2000464  5750 8087 12606 13237 24654 31231 47626 48299\n",
       "471    417305                               8048 21903 47226\n",
       "495   1614375                    2846 2966 23734 25513 47766"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at an example of an output\n",
    "df_ohe_validation.set_submission()\n",
    "df_ohe_validation.submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting ready\n",
    "test_users = pd.Series(df_test.user_id.unique())\n",
    "all_test_users = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "missing_test_users = all_test_users.loc[~all_test_users.isin(test_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create user batches\n",
    "batch_size = 1024\n",
    "user_batches = []\n",
    "\n",
    "while test_users.size > 0:\n",
    "    batch = test_users[:batch_size]\n",
    "    test_users = test_users[batch_size:]\n",
    "    user_batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there may be some missing users who did not purchase\n",
    "# any product from the bestsellers list\n",
    "# we need to keep track of them to add them back in\n",
    "# in the final result at the end\n",
    "missing_test_users_submission = pd.DataFrame(index=missing_test_users)\n",
    "missing_test_users_submission['order_id'] = user_to_order\n",
    "missing_test_users_submission['products'] = 'None'\n",
    "\n",
    "# matching index data type to DF_ohe.submission.index\n",
    "missing_test_users_submission.index = missing_test_users_submission.index.astype(np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 \n",
      "Wall time: 17min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "submission = pd.DataFrame(columns=['order_id', 'products'])\n",
    "submission.index = submission.index.astype(np.uint64)\n",
    "\n",
    "for batch in user_batches:\n",
    "    print(str(i), end=' ')\n",
    "    i += 1\n",
    "    df_ohe_test = DF_ohe(df_test, batch)\n",
    "    df_ohe_test.set_predict(model)\n",
    "    df_ohe_test.set_submission()\n",
    "    submission = submission.append(df_ohe_test.submission)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding in missing users and sort the result by order_id\n",
    "submission = submission.append(missing_test_users_submission)\n",
    "submission = submission.sort_values(by='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export to a file\n",
    "submission.to_csv(model_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for multiple trials, if we wanted to be more rigorous,\n",
    "# on each trial, we should be taking new samples for fitting\n",
    "# however, we do not implement this as this would take much more time to process\n",
    "def model_test(model, model_name, sample_size=128, trials=1, verbose=False, pickle=False):\n",
    "    print('================================================================')\n",
    "    print('TS{:05d}_'.format(sample_size) + model_name)\n",
    "    print('================================================================')\n",
    "    \n",
    "    # get a sample and a fit\n",
    "    sample_users = get_sample_users(df_train, sample_size)\n",
    "    df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "    df_ohe_sample.set_target(df_train_target)\n",
    "    \n",
    "    %time model.fit(df_ohe_sample.sparse_features, df_ohe_sample.sparse_target)\n",
    "    \n",
    "    # pickle the model for later use if pickle is true\n",
    "    if pickle:\n",
    "        joblib.dump(model, 'TS{:05d}_'.format(sample_size) + model_name + '.pkl')\n",
    "    \n",
    "    for i in range(trials):\n",
    "        print('Trial #{}'.format(i + 1))\n",
    "        print('--------------------------------')\n",
    "        \n",
    "        # validate model on a new sample\n",
    "        sample_validation_users = get_sample_users(df_train, sample_size)\n",
    "        df_ohe_validation = DF_ohe(df_train, sample_validation_users)\n",
    "        df_ohe_validation.set_target(df_train_target)\n",
    "        \n",
    "        %time df_ohe_validation.set_predict(model)\n",
    "        \n",
    "        # print results\n",
    "        df_ohe_validation.print_results(verbose)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "TS00128_OVRDT\n",
      "================================================================\n",
      "Wall time: 23.7 s\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 422 ms\n",
      "precision score: 0.137\n",
      "pseudo f1 score: 0.069\n",
      "\n",
      "================================================================\n",
      "TS00128_OVRDTSL5\n",
      "================================================================\n",
      "Wall time: 21.2 s\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 1.02 s\n",
      "precision score: 0.489\n",
      "pseudo f1 score: 0.060\n",
      "\n",
      "================================================================\n",
      "TS00128_OVRDTSS5\n",
      "================================================================\n",
      "Wall time: 28.1 s\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 407 ms\n",
      "precision score: 0.145\n",
      "pseudo f1 score: 0.073\n",
      "\n",
      "================================================================\n",
      "TS00128_OVRDTMD5\n",
      "================================================================\n",
      "Wall time: 17.6 s\n",
      "Trial #1\n",
      "--------------------------------\n",
      "Wall time: 411 ms\n",
      "precision score: 0.157\n",
      "pseudo f1 score: 0.076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "# setting test arguments\n",
    "trials = 1\n",
    "verbose = False\n",
    "pickle = False\n",
    "sample_sizes = [128]\n",
    "models = [\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier()), 'OVRDT'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(min_samples_leaf=5)), 'OVRDTSL5'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(min_samples_split=5)), 'OVRDTSS5'),\n",
    "    (OneVsRestClassifier(DecisionTreeClassifier(max_depth=5)), 'OVRDTMD5'),\n",
    "]\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    for model, model_name in models:\n",
    "        model_test(model, model_name, sample_size, trials, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do\n",
    "----\n",
    "* add more features (order history)\n",
    "* optimize parameters\n",
    "* append missing users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Incremental Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encode the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
