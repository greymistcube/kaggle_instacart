{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aisles = pd.read_csv(path + 'aisles.csv')\n",
    "df_departments = pd.read_csv(path + 'departments.csv')\n",
    "df_data = pd.read_csv(path + 'order_products__prior.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'product_id': np.int32,\n",
    "    'add_to_cart_order': np.int16,\n",
    "    'reordered': np.int8,\n",
    "})\n",
    "df_train_target = pd.read_csv(path + 'order_products__train.csv')\n",
    "df_orders = pd.read_csv(path + 'orders.csv', dtype={\n",
    "    'order_id': np.int32,\n",
    "    'user_id': np.int32,\n",
    "    'order_number': np.int8,\n",
    "    'order_dow': np.int8,\n",
    "    'order_hour_of_day': np.int8,\n",
    "})\n",
    "df_products = pd.read_csv(path + 'products.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge arguments we will be using often\n",
    "merge_arguments = {\n",
    "    'left_index': True,\n",
    "    'right_index': True,\n",
    "    'how': 'outer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting index for easier mapping\n",
    "df_orders = df_orders.set_index('order_id')\n",
    "df_products = df_products.set_index('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# adding features to data to make it more complete\n",
    "# many of the values are repeated as a single user places many orders with many products\n",
    "df_data['user_id'] = df_data.order_id.map(df_orders.user_id)\n",
    "df_data['order_number'] = df_data.order_id.map(df_orders.order_number)\n",
    "df_data['order_dow'] = df_data.order_id.map(df_orders.order_dow)\n",
    "df_data['order_hour_of_day'] = df_data.order_id.map(df_orders.order_hour_of_day)\n",
    "df_data['days_since_prior_order'] = df_data.order_id.map(df_orders.days_since_prior_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of users in each group\n",
    "users_train = df_orders.loc[df_orders.eval_set == 'train', 'user_id']\n",
    "users_test = df_orders.loc[df_orders.eval_set == 'test', 'user_id']\n",
    "\n",
    "# create maps for convenience\n",
    "user_to_last_order = df_data.groupby('user_id').agg({'order_number': 'max'}).order_number\n",
    "order_to_user = df_orders.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into two groups\n",
    "df_train = df_data.loc[df_data.user_id.isin(users_train)]\n",
    "df_test = df_data.loc[df_data.user_id.isin(users_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data: (32434489, 9)\n",
      "# of train data: (20641991, 9)\n",
      "# of test data: (11792498, 9)\n"
     ]
    }
   ],
   "source": [
    "# a quick look at the sizes\n",
    "print(\"# of data: {}\".format(df_data.shape))\n",
    "print(\"# of train data: {}\".format(df_train.shape))\n",
    "print(\"# of test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bestsellers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_sellers(df, quantile):\n",
    "    df_temp = df.groupby('product_id').agg({'order_id': 'count'}).rename(columns={'order_id': 'amount_sold'})\n",
    "    return df_temp.loc[df_temp.amount_sold >= df_temp.amount_sold.quantile(quantile)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we have seen in part 02,\n",
    "# top 20% of most sold products account for more than 90% of all items sold\n",
    "# there are approximately 50,000 different products sold\n",
    "# this should cut down the number of features down to about 10,000\n",
    "# as we shall be using one-hot encoded data for our model\n",
    "quantile = 0.8\n",
    "\n",
    "bestsellers = get_best_sellers(df_data, quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only bestsellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.product_id.isin(bestsellers)]\n",
    "df_test = df_test.loc[df_test.product_id.isin(bestsellers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in train data: (18785336, 9)\n",
      "# of bestsellers in test data: (10723251, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in train data: {}\".format(df_train.shape))\n",
    "print(\"# of bestsellers in test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only last 3 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the minimum number of orders made by a user is 3\n",
    "# we cut off all orders prior to the last three\n",
    "# this is mainly done to have constant number of features across all users\n",
    "# after one-hot encoding all (products, order number) pairs\n",
    "# hopefully last 3 orders are relavent enough in predicting reorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not, we may do more feature engineering later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_orders(df):\n",
    "    return df.loc[df.user_id.map(user_to_last_order) - df.order_number < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = get_last_orders(df_train)\n",
    "df_test = get_last_orders(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bestsellers in the last 3 train data: (3622674, 9)\n",
      "# of bestsellers in the last 3 test data: (2075829, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"# of bestsellers in the last 3 train data: {}\".format(df_train.shape))\n",
    "print(\"# of bestsellers in the last 3 test data: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function reduces order number down to 0, 1, or 2\n",
    "# these numbers are \"relative\" to the last order number made by a user\n",
    "# this is done so that we may one-hot encode the feature\n",
    "def standardize_order_number(df):\n",
    "    df.order_number = df.order_number - (df.user_id.map(user_to_last_order) - 2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded product feature and reordered feature\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id, prefix='prod'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.product_id * df.reordered, prefix='re'), **merge_arguments)\n",
    "    \n",
    "    # group by order_number so that each row contains all the information\n",
    "    # on which products are ordered in that particular order\n",
    "    # we lose the information on when the products are added to the cart on that particular order\n",
    "    # but this information may not be relavent\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).sum()\n",
    "    \n",
    "    # unstack order_numbers so that each row now contains all the information\n",
    "    # on which products are ordered by a user in the last 3 orders\n",
    "    # fill_value is needed as there may be some users\n",
    "    # who did not order one of the bestsellers in a particular order among the last 3 orders\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_grouped_by_user(df):\n",
    "    # setting up an \"empty dataframe\" to merge one-hot encoded features\n",
    "    df_temp = pd.DataFrame(index=df.index)\n",
    "    df_temp['user_id'] = df.user_id\n",
    "    df_temp['order_number'] = df.order_number\n",
    "    \n",
    "    # merge one-hot encoded time related features\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_dow, prefix='dow'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.order_hour_of_day, prefix='hour'), **merge_arguments)\n",
    "    df_temp = df_temp.merge(pd.get_dummies(df.days_since_prior_order, prefix='days', dummy_na=True), **merge_arguments)\n",
    "    \n",
    "    # similar as above\n",
    "    df_temp = df_temp.groupby(['user_id', 'order_number']).max()\n",
    "    df_temp = df_temp.unstack(fill_value=0)\n",
    "    df_temp.columns = ['_'.join([str(col[1]), str(col[0])]) for col in df_temp.columns]\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function that returns column names in a predefined format\n",
    "# to make sure that we have a well defined format of a full list of features\n",
    "# otherwise, when using a partial set of users for our input,\n",
    "# we might be missing certain products\n",
    "def get_product_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        np.core.defchararray.add(str(i) + infix, bestsellers.astype(str))\n",
    "        for i in range(3) for infix in ['_prod_', '_re_']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_ohe_columns():\n",
    "    return np.concatenate([\n",
    "        [str(i) + '_dow_' + str(dow) for dow in range(7) for i in range(3)],\n",
    "        [str(i) + '_hour_' + str(hour) for hour in range(24) for i in range(3)],\n",
    "        [str(i) + '_days_' + str(days) for days in np.sort(df_orders.days_since_prior_order.unique()) for i in range(3)],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_columns():\n",
    "    return np.concatenate([get_time_ohe_columns(), get_product_ohe_columns()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple function to combine one-hot encoded product features and time related features\n",
    "def get_ohe_features(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=get_ohe_columns(), dtype=np.uint8)\n",
    "    \n",
    "    df_temp = get_product_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp)] = df_temp\n",
    "    \n",
    "    df_temp = get_time_ohe_grouped_by_user(df)\n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp)] = df_temp\n",
    "    \n",
    "    # add one last feature\n",
    "    # we use log for this part (see part 02)\n",
    "    df_ohe['last_order'] = np.log(user_to_last_order)\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_product_ohe_target(df, users):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['user_id'] = df_temp.order_id.map(order_to_user)\n",
    "    \n",
    "    df_temp = df_temp.loc[df_temp.user_id.isin(users)]\n",
    "    \n",
    "    # take only those that are reorders of bestsellers\n",
    "    df_temp = df_temp.loc[df_temp.reordered == 1]\n",
    "    df_temp = df_temp.loc[df_temp.product_id.isin(bestsellers)]\n",
    "    \n",
    "    df_temp = df_temp.merge(pd.get_dummies(df_temp.product_id), **merge_arguments)\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    df_temp = df_temp.drop(['order_id', 'product_id', 'add_to_cart_order', 'reordered'], axis=1)\n",
    "    \n",
    "    # group by users\n",
    "    df_temp = df_temp.groupby('user_id').sum()\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ohe_target(df, users):\n",
    "    df_ohe = pd.DataFrame(data=0, index=users, columns=bestsellers, dtype=np.uint8)\n",
    "    df_temp = get_product_ohe_target(df, users)\n",
    "    \n",
    "    # add possible missing users back in to the target\n",
    "    missing_users = users.loc[~users.isin(df_temp.index)]\n",
    "    df_temp = df_temp.append([pd.DataFrame(data=0, index=missing_users, columns=df_temp.columns, dtype=np.uint8)])\n",
    "    \n",
    "    df_ohe.loc[:, df_ohe.columns.isin(df_temp.columns)] = df_temp\n",
    "    \n",
    "    return df_ohe.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a sample of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_users(df, n):\n",
    "    return pd.Series(np.random.choice(df.user_id.unique(), n, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "sample_users = get_sample_users(df_train, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DF_ohe:\n",
    "    def __init__(self, df, users):\n",
    "        self.df = df.loc[df.user_id.isin(users)].copy()\n",
    "        self.df = standardize_order_number(self.df)\n",
    "        \n",
    "        # possible missing users\n",
    "        # sample is take after dataframe reduction\n",
    "        # we need to account for that\n",
    "        self.features = get_ohe_features(self.df, users)\n",
    "        \n",
    "    def set_target(self, df, users):\n",
    "        self.target = get_ohe_target(df, users)\n",
    "    \n",
    "    def set_predict(self, df):\n",
    "        self.predict = df\n",
    "    \n",
    "    def to_submission_form(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_sample = DF_ohe(df_train, sample_users)\n",
    "df_ohe_sample.set_target(df_train_target, sample_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,969,000\n"
     ]
    }
   ],
   "source": [
    "print('{:,}'.format(df_ohe_sample.features.memory_usage().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if time and memory used are in linear relation to ths number of users,\n",
    "# as there are about 130,000 users in the training data,\n",
    "# this should take little more than an hour\n",
    "# and should take up about 60GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparsifying the dataframes takes more than 2 minutes\n",
    "# for a sample of 100 users\n",
    "# although this reduces the memory usage down to 1/50 of\n",
    "# its dense form, this would take more than 2 days to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# besides, due to pandas not being able to merge sparse data properly\n",
    "# (i.e., even with sparse dataframe as its input, its output is always a dense dataframe)\n",
    "# memory usage is still an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append missing users\n",
    "--------\n",
    "There may be missing users after the whole process. Namely, those who did not order any products from bestsellers list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "model.fit(df_ohe_sample.features, df_ohe_sample.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a sample size of 100\n",
    "# OvR + SGDClassifier:\n",
    "#   fit: 64s predict: 33s score: 0.072 0.063 0.050\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   fit: 29s predict: 22s score: 0.099 0.064 0.096\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   fit: 126s predict: 38s score: 0.070, 0.112, 0.066\n",
    "# DecisionTreeClassifier:\n",
    "#   fit: 30s predict: 0s score: 0.055 0.036 0.080\n",
    "\n",
    "# for a sample size of 200\n",
    "# OvR + SGDClassifier: fit:\n",
    "#   fit: 221s predict: 130s score: 0.076 0.092 0.131\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   fit: 131s predict: 88s score: 0.150 0.119 0.151\n",
    "# OvR + AdaBoostClassifier:\n",
    "#   fit: 534s predict: 149s score: 0.115 0.095 0.090\n",
    "# DecisionTreeClassifier:\n",
    "#   fit: 109s predict: 0s score: 0.057 0.041 0.074\n",
    "\n",
    "# for a sample size of 400\n",
    "# OvR + DecisionTreeClassifier:\n",
    "#   fit: 863s predict: 506s score: 0.125 0.084 0.084\n",
    "#   comment: there seems to be an overfitting\n",
    "\n",
    "# OvR + MLPClassifier: takes too long\n",
    "# MLPClassifier: fail\n",
    "# OvR + LinearSVC: fail\n",
    "# OvR + RandomForestClassifier: fail\n",
    "# RandomForestClassifier: fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(data=model.predict(df_ohe_sample.features),\n",
    "                  index=df_ohe_sample.features.index, columns=bestsellers, dtype=np.uint8)\n",
    "df_ohe_sample.set_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reorders in target: 1316\n",
      "# of reorders in predict: 1317\n"
     ]
    }
   ],
   "source": [
    "print('# of reorders in target: {}'.format(df_ohe_sample.target.sum().sum()))\n",
    "print('# of reorders in predict: {}'.format(df_ohe_sample.predict.sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_ohe_sample.predict == df_ohe_sample.target).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a sample of users\n",
    "validation_size = 400\n",
    "sample_validation_users = get_sample_users(df_train, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ohe_validation = DF_ohe(df_train, sample_validation_users)\n",
    "df_ohe_validation.set_target(df_train_target, sample_validation_users)\n",
    "df = pd.DataFrame(data=model.predict(df_ohe_validation.features),\n",
    "                  index=df_ohe_validation.features.index, columns=bestsellers, dtype=np.uint8)\n",
    "df_ohe_validation.set_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of predicted reorders\n",
    "PT = df_ohe_validation.predict.sum().sum()\n",
    "# number of relevant reorders\n",
    "RT = df_ohe_validation.target.sum().sum()\n",
    "# number of false negative among bestsellers\n",
    "FN1 = (df_ohe_validation.predict != df_ohe_validation.target).sum().sum()\n",
    "# estimation of false negative among non-bestsellers\n",
    "FN2 = 0.1 * RT\n",
    "# true positive of reorders\n",
    "TP = PT + RT - FN1\n",
    "# false positive of reorders\n",
    "FP = PT - TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted true: 226\n",
      "relevant true: 2506\n",
      "true positive: 88\n",
      "false negative 1: 2644\n",
      "false negative 2: 250.60000000000002\n",
      "false positive: 138\n"
     ]
    }
   ],
   "source": [
    "print('predicted true: {}'.format(PT))\n",
    "print('relevant true: {}'.format(RT))\n",
    "print('true positive: {}'.format(TP))\n",
    "print('false negative 1: {}'.format(FN1))\n",
    "print('false negative 2: {}'.format(FN2))\n",
    "print('false positive: {}'.format(FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo f1 score: 0.055\n"
     ]
    }
   ],
   "source": [
    "print('pseudo f1 score: {:.3f}'.format((2 * TP) / ((2 * TP) + FN1 + FN2 + FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Incremental Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encode the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
